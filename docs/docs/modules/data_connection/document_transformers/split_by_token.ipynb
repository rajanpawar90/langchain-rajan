{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05c860c",
   "metadata": {},
   "source": [
    "# Split Text into Chunks\n",
    "\n",
    "Language models have a token limit. To avoid exceeding this limit, it's a good idea to split text into smaller chunks. This notebook demonstrates how to split text using different tokenizers and methods.\n",
    "\n",
    "**Note**: Make sure to install the required packages before running the code.\n",
    "```bash\n",
    "pip install -U langchain-text-splitters tiktoken spacy nltk konlpy transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683b36a",
   "metadata": {},
   "source": [
    "## tiktoken\n",
    "\n",
    "`tiktoken` is a fast BPE tokenizer created by OpenAI. It is more accurate for estimating tokens used in OpenAI models.\n",
    "\n",
    "1. Text splitting: by character passed in.\n",
    "2. Chunk size measurement: by tiktoken tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ef83e-f43a-465
