# OpenVINO Embeddings Demo
# This notebook demonstrates how to use OpenVINO for Hugging Face model embedding.

# Install required packages
# Note: you may need to restart the kernel to use updated packages.
%pip install --upgrade-strategy eager "optimum[openvino,nncf]" --quiet

# Import necessary modules
import json
import os
from pathlib import Path
from typing import Dict, List, Any

import numpy as np
import torch
from IPython.display import Markdown, display
from openvino.runtime import Core
from torch import Tensor

# Define a function to install and import OpenVINOEmbeddings
def import_openvino_embeddings(ov_core: Core) -> None:
    try:
        from langchain_community.embeddings import OpenVINOEmbeddings
    except ImportError:
        print("Error: langchain_community package not found. Installing...")
        !pip install langchain-community[openvino] --quiet
        from langchain_community.embeddings import OpenVINOEmbeddings

# Initialize OpenVINO runtime
ov_core = Core()
import_openvino_embeddings(ov_core)

# Define a function to load the model
def load_model(model_name: str, model_kwargs: Dict[str, Any]) -> Any:
    try:
        model = OpenVINOEmbeddings(model_name_or_path=model_name, **model_kwargs)
    except Exception as e:
        print(f"Error: Unable to load model ({model_name}): {e}")
        return None
    return model

# Define a function to embed text
def embed_text(model: Any, text: str, encode_kwargs: Dict[str, Any]) -> Tensor:
    try:
        query_result = model.embed_query(text, **encode_kwargs)
    except Exception as e:
        print(f"Error: Unable to embed text: {e}")
        return None
    return query_result

# Define a function to save the model to OpenVINO IR format
def save_model_to_ir(model: Any, model_name: str) -> None:
    try:
        model_dir = Path(model_name)
        model_dir.mkdir(parents=True, exist_ok=True)
        model.save_model(model_dir)
    except Exception as e:
        print(f"Error: Unable to save model to IR format: {e}")

# Define a function to load the model from OpenVINO IR format
def load_model_from_ir(model_name: str, model_kwargs: Dict[str, Any]) -> Any:
    try:
        model = OpenVINOEmbeddings(model_name_or_path=model_name, **model_kwargs)
    except Exception as e:
        print(f"Error: Unable to load model from IR format ({model_name}): {e}")
        return None
    return model

# Define the models and their parameters
model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "CPU"}
encode_kwargs = {"mean_pooling": True, "normalize_embeddings": True}

# Load the model
model = load_model(model_name, model_kwargs)
if model is None:
    print("Error: Unable to load the model. Exiting...")
    exit(1)

# Embed text
text = "This is a test document."
query_result = embed_text(model, text, encode_kwargs)
if query_result is None:
    print("Error: Unable to embed text. Exiting...")
    exit(1)

# Display the embedded text
display(Markdown(f"Embedded text:\n{json.dumps(query_result.tolist(), indent=2)}"))

# Save the model to OpenVINO IR format
save_model_to_ir(model, "all-mpnet-base-v2-ov")

# Load the model from OpenVINO IR format
model_from_ir = load_model_from_ir("all-mpnet-base-v2-ov", model_kwargs)
if model_from_ir is None:
    print("Error: Unable to load the model from IR format. Exiting...")
    exit(1)

# Embed text using the model from OpenVINO IR format
query_result_ir = embed_text(model_from_ir, text, encode_kwargs)
if query_result_ir is None:
    print("Error: Unable to embed text using the model from OpenVINO IR format. Exiting...")
    exit(1)

# Compare the results
if np.allclose(query_result, query_result_ir):
    print("Embeddings from the original and IR-formatted models match.")
else:
    print("Embeddings from the original and IR-formatted models do not match.")

