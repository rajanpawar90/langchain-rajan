{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700a516b",
   "metadata": {},
   "source": [
    "# OpenAI Adapter(Improved)\n",
    "\n",
    "**Please ensure OpenAI library is less than 1.0.0; otherwise, refer to the newer doc [OpenAI Adapter](/docs/integrations/adapters/openai/).**\n",
    "\n",
    "A lot of people get started with OpenAI but want to explore other models. LangChain's integrations with many model providers make this easy to do so. While LangChain has it's own message and model APIs, we've also made it as easy as possible to explore other models by exposing an adapter to adapt LangChain models to the OpenAI api.\n",
    "\n",
    "At the moment this only deals with output and does not return other information (token counts, stop reasons, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import openai\n",
    "    from langchain_community.adapters import openai as lc_openai\n",
    "except ImportError as e:\n",
    "    print(\"Error importing required libraries. Please install them and try again.\")\n",
    "    print(e)\n",
    "    raise\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522ceda",
   "metadata": {},
   "source": [
    "## ChatCompletion.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_completion(messages, model='gpt-3.5-turbo', temperature=0, provider='OpenAI'):\n",
    "    if provider == 'OpenAI':\n",
    "        result = openai.ChatCompletion.create(\n",
    "            messages=messages, model=model, temperature=temperature\n",
    "        )\n",
    "    elif provider == 'ChatAnthropic':\n",
    "        result = lc_openai.ChatCompletion.create(\n",
    "            messages=messages, model=model, temperature=temperature, provider=provider\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Invalid provider: {provider}')\n",
    "\n",
    "    return result['choices'][0]['message']\n",
    "\n",
    "def print_chat_completion(completion):\n",
    "    print(f'Role: {completion["role"]}, Content: {completion["content"]}')\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'hi'}]\n",
    "\n",
    "completion = create_chat_completion(messages, model='gpt-3.5-turbo', temperature=0, provider='OpenAI')\n",
    "print_chat_completion(completion)\n",
    "\n",
    "completion = create_chat_completion(messages, model='gpt-3.5-turbo', temperature=0, provider='ChatAnthropic')\n",
    "print_chat_completion(completion)\n",
    "\n",
    "completion = create_chat_completion(messages, model='claude-2', temperature=0, provider='ChatAnthropic')\n",
    "print_chat_completion(completion)\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ba845",
   "metadata": {},
   "source": [
    "## ChatCompletion.stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8cb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat_completion_stream(stream):\n",
    "    for c in stream:\n",
    "        if 'content' in c['choices'][0]['delta']:\n",
    "            print(f'Role: {c['choices'][0]['message']['role']}, Content: {c['choices'][0]['delta']['content']}')\n",
    "\n",
    "def create_chat_completion_stream(messages, model='gpt-3.5-turbo', temperature=0, provider='OpenAI'):\n",
    "    if provider == 'OpenAI':\n",
    "        stream = openai.ChatCompletion.create(\n",
    "            messages=messages, model=model, temperature=temperature, stream=True\n",
    "        )\n",
    "    elif provider == 'ChatAnthropic':\n",
    "        stream = lc_openai.ChatCompletion.create(\n",
    "            messages=messages, model=model, temperature=temperature, provider=provider, stream=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Invalid provider: {provider}')\n",
    "\n",
    "    return stream\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'hi'}]\n",
    "\n",
    "stream = create_chat_completion_stream(messages, model='gpt-3.5-turbo', temperature=0, provider='OpenAI')\n",
    "print_chat_completion_stream(stream)\n",
    "\n",
    "stream = create_chat_completion_stream(messages, model='gpt-3.5-turbo', temperature=0, provider='ChatAnthropic')\n",
    "print_chat_completion_stream(stream)\n",
    "\n",
    "stream = create_chat_completion_stream(messages, model='claude-2', temperature=0, provider='ChatAnthropic')\n",
    "print_chat_completion_stream(stream)\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
