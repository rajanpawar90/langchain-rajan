# Criteria Evaluation

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/guides/evaluation/string/criteria_eval_chain.ipynb)

In this notebook, you will learn how to use the `CriteriaEvalChain` to assess a model's output based on a specific rubric or criteria set. This can be useful for verifying if an LLM or Chain's output complies with a defined set of criteria.

## Usage without references

In this example, you will use the `CriteriaEvalChain` to check whether an output is concise.

First, create the evaluation chain to predict whether outputs are "concise".




Now, evaluate a sample prediction.




### Output Format

All string evaluators expose an `evaluate_strings` method, which accepts:

- `input` (str): The input to the agent.
- `prediction` (str): The predicted response.

The criteria evaluators return a dictionary with the following values:

- `score`: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise.
- `value`: A "Y" or "N" corresponding to the score.
- `reasoning`: String "chain of thought reasoning" from the LLM generated prior to creating the score.

## Using Reference Labels

Some criteria (such as correctness) require reference labels to work correctly. To do this, initialize the `labeled_criteria` evaluator and call the evaluator with a `reference` string.




## Custom Criteria

To evaluate outputs against your own custom criteria, pass in a dictionary of `"criterion_name": "criterion_description"`.




## Using Constitutional Principles

Custom rubrics are similar to principles from Constitutional AI. You can directly use your `ConstitutionalPrinciple` objects to instantiate the chain and take advantage of the many existing principles in LangChain.




## Configuring the LLM

If you don't specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. You can use a different LLM by passing it as an argument.




## Configuring the Prompt

If you want to completely customize the prompt, you can initialize the evaluator with a custom prompt template.



